# ğŸ“¡ Telecom Big-Data Pipeline â€“ Mediation â†’ Rating â†’ Billing â†’ Reporting

> **Goal:** Convert raw network usages (CDR/EDR) into reliable revenue and business KPIs  
> **Stack:** Kafka | Apache Spark (Structured Streaming + Batch) | PostgreSQL | Parquet | Power BI  

---

## ğŸŒ End-to-End Architecture

```mermaid
flowchart LR
  A[Kafka<br>CDR topic] --> B(Spark Streaming<br>Mediation & Clean)
  B -->|Parquet /record_type| C[cleaned_cdrs]
  C --> D(Spark Batch<br>Rating Engine)
  subgraph PostgreSQL
    E[customers]
  end
  subgraph ressources
    F[rate_plans]
    G[product_catlog]
  end
  D --JDBC--> E
  D --CSV--> F
  D --CSV--> G
  D -->|Parquet| H[rated_cdrs/]
  H --> I(Spark Batch<br>Billing Engine)
  I -->|Parquet| J[facturation_complete/]
  J --> K[Power BI Dashboard]
```
## ğŸ“‚ Project Layout

â”œâ”€ conf/                                         # JARs
â”œâ”€ src/
â”‚  â”œâ”€ Synthetic_Data_Generation.py               # Faker CDR producer â†’ Kafka
â”‚  â”œâ”€ Streaming-Based_Mediation.py               # Spark Structured Streaming
â”‚  â”œâ”€ RatingEngine.ipynb                         # Spark Batch (Rating Engine)
â”‚  â”œâ”€ BillingEngine.ipynb                        # Spark Batch (Billing Engine)
â”‚  â”œâ”€ kafka_streaming.py                         # Kafka Producer
â”‚  â”œâ”€ initaisation_pg.py                         # Initialization en postgresql
â”‚  â””â”€ reporting.ipynb                            # Spark job -> CSV for BI
â”œâ”€ resources/
â”‚  â”œâ”€ product_catalog.csv
â”‚  â””â”€ rate_plans.csv
â”œâ”€ cleaned_cdrs/                                 # output MÃ©diation (Parquet)
â”œâ”€ rated_cdrs/                                   # output Rating     (Parquet)
â”œâ”€ billing/
â”‚  â””â”€ facturation_complete/                      # output Billing   (Parquet)
â””â”€ report/                                       # CSV prÃªts pour Power BI
