# ğŸ“¡ Telecom Big-Data Pipeline â€“ Mediation â†’ Rating â†’ Billing â†’ Reporting

> **Goal:** Convert raw network usages (CDR/EDR) into reliable revenue and business KPIs  
> **Stack:** Kafka | Apache Spark (Structured Streaming + Batch) | PostgreSQL | Parquet | Power BI  

---

## ğŸŒ End-to-End Architecture

```mermaid
flowchart LR
  A[Kafka<br>CDR topic] --> B(Spark Streaming<br>Mediation & Clean)
  B -->|Parquet /record_type| C[cleaned_cdrs]
  C --> D(Spark Batch<br>Rating Engine)
  subgraph PostgreSQL
    E[customers]
  end
  subgraph ressources
    F[rate_plans]
    G[product_catlog]
  end
  D --JDBC--> E
  D --CSV--> F
  D --CSV--> G
  D -->|Parquet| H[rated_cdrs/]
  H --> I(Spark Batch<br>Billing Engine)
  I -->|Parquet| J[facturation_complete/]
  J --> K[Power BI Dashboard]
```
## ğŸ“‚ Project Layout

project-root/
â”œâ”€ conf/                                         # JARs  
â”œâ”€ src/  
â”‚  â”œâ”€ Synthetic_Data_Generation.py               # GÃ©nÃ©rateur de CDR factices â†’ Kafka  
â”‚  â”œâ”€ Streaming_Mediation.py                     # MÃ©diation Spark Structured Streaming  
â”‚  â”œâ”€ RatingEngine.ipynb                         # Spark Batch (Rating Engine)  
â”‚  â”œâ”€ BillingEngine.ipynb                        # Spark Batch (Billing Engine)  
â”‚  â”œâ”€ kafka_streaming.py                         # Kafka Producer  
â”‚  â”œâ”€ initialisation_pg.py                       # Initialisation PostgreSQL  
â”‚  â””â”€ reporting.ipynb                            # Job Spark â†’ CSV pour BI  
â”œâ”€ resources/  
â”‚  â”œâ”€ product_catalog.csv  
â”‚  â””â”€ rate_plans.csv  
â”œâ”€ cleaned_cdrs/                                 # Sortie MÃ©diation (Parquet)  
â”œâ”€ rated_cdrs/                                   # Sortie Rating (Parquet)  
â”œâ”€ billing/  
â”‚  â””â”€ facturation_complete/                      # Sortie Facturation (Parquet)  
â””â”€ report/                                       # CSV prÃªts pour Power BI
